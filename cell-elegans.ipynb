{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q optuna\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-09T06:28:37.735365Z","iopub.execute_input":"2023-08-09T06:28:37.735674Z","iopub.status.idle":"2023-08-09T06:28:51.767610Z","shell.execute_reply.started":"2023-08-09T06:28:37.735649Z","shell.execute_reply":"2023-08-09T06:28:51.766429Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install  -q monai\n","metadata":{"execution":{"iopub.status.busy":"2023-08-09T06:28:51.770188Z","iopub.execute_input":"2023-08-09T06:28:51.770593Z","iopub.status.idle":"2023-08-09T06:29:04.776098Z","shell.execute_reply.started":"2023-08-09T06:28:51.770554Z","shell.execute_reply":"2023-08-09T06:29:04.774838Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -q albumentations","metadata":{"execution":{"iopub.status.busy":"2023-08-09T06:29:04.778893Z","iopub.execute_input":"2023-08-09T06:29:04.779293Z","iopub.status.idle":"2023-08-09T06:29:16.106796Z","shell.execute_reply.started":"2023-08-09T06:29:04.779253Z","shell.execute_reply":"2023-08-09T06:29:16.105515Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -q git+https://github.com/huggingface/transformers.git","metadata":{"execution":{"iopub.status.busy":"2023-08-09T06:29:16.110746Z","iopub.execute_input":"2023-08-09T06:29:16.111165Z","iopub.status.idle":"2023-08-09T06:30:02.036200Z","shell.execute_reply.started":"2023-08-09T06:29:16.111123Z","shell.execute_reply":"2023-08-09T06:30:02.034817Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os,sys\nimport numpy as np \nimport pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt\nfrom PIL import Image \nfrom tqdm.notebook import tqdm\nfrom IPython.display import clear_output","metadata":{"execution":{"iopub.status.busy":"2023-08-09T06:30:02.037867Z","iopub.execute_input":"2023-08-09T06:30:02.038594Z","iopub.status.idle":"2023-08-09T06:30:02.515378Z","shell.execute_reply.started":"2023-08-09T06:30:02.038547Z","shell.execute_reply":"2023-08-09T06:30:02.514391Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#all required torch libraries\nimport torch\nimport torchvision\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader,Dataset\nfrom torchvision import transforms,utils\nimport torchvision.transforms as transforms\nfrom torchvision.transforms import ToTensor\nfrom torchvision.transforms import ToPILImage\nfrom torch.utils.data.dataset import random_split\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torchvision.ops import masks_to_boxes\nfrom albumentations import Compose, Transpose, VerticalFlip, HorizontalFlip, RandomRotate90, ShiftScaleRotate, OpticalDistortion","metadata":{"execution":{"iopub.status.busy":"2023-08-09T06:30:02.517331Z","iopub.execute_input":"2023-08-09T06:30:02.518084Z","iopub.status.idle":"2023-08-09T06:30:07.428686Z","shell.execute_reply.started":"2023-08-09T06:30:02.518050Z","shell.execute_reply":"2023-08-09T06:30:07.427707Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"\nimport time\nimport optuna\nimport csv\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(\"finished\")\nfrom os import listdir\nfrom os.path import isfile, join","metadata":{"execution":{"iopub.status.busy":"2023-08-09T06:30:07.430275Z","iopub.execute_input":"2023-08-09T06:30:07.430639Z","iopub.status.idle":"2023-08-09T06:30:07.942989Z","shell.execute_reply.started":"2023-08-09T06:30:07.430604Z","shell.execute_reply":"2023-08-09T06:30:07.942057Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"finished\n","output_type":"stream"}]},{"cell_type":"code","source":"import monai","metadata":{"execution":{"iopub.status.busy":"2023-08-09T06:30:07.944312Z","iopub.execute_input":"2023-08-09T06:30:07.944758Z","iopub.status.idle":"2023-08-09T06:30:18.486635Z","shell.execute_reply.started":"2023-08-09T06:30:07.944718Z","shell.execute_reply":"2023-08-09T06:30:18.485655Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"def resize(path):\n  dirs = os.listdir( path )\n  for item in tqdm(dirs):\n    if os.path.isfile(path+item):\n      im = Image.open(path+item)\n      f, e = os.path.splitext(path+item)\n      imResize = im.resize((1024,1024), Image.NEAREST)\n      imResize.save(f+e, 'PNG', quality=100)\n      \nlabel_path =  \"/kaggle/input/nucleus-data-c-elegans/nucleus_data/segmentation_maps\"\noutput_features_path = \"/kaggle/input/nucleus-data-c-elegans/nucleus_data/features\"\nresize(label_path)","metadata":{"execution":{"iopub.status.busy":"2023-08-09T06:30:18.488178Z","iopub.execute_input":"2023-08-09T06:30:18.488976Z","iopub.status.idle":"2023-08-09T06:30:22.000500Z","shell.execute_reply.started":"2023-08-09T06:30:18.488940Z","shell.execute_reply":"2023-08-09T06:30:21.998333Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/6790 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf718f326bb2478399f152a9458bc39f"}},"metadata":{}}]},{"cell_type":"code","source":"ids=[]\nlabel_filenames = [f for f in listdir(label_path) if isfile(join(label_path, f))]\nfeature_filenames = [f for f in listdir(output_features_path) if isfile(join(output_features_path, f))]\nfor i in range(len(feature_filenames)):\n  ids.append(feature_filenames[i][1:])\nprint(len(ids))\n\ndf = pd.DataFrame(ids ,columns=[\"file_ids\"])\ndf.to_csv('full_file_ids.csv', index=False)\n\n#sanity check\ndf = pd.read_csv('full_file_ids.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-09T06:30:22.002233Z","iopub.execute_input":"2023-08-09T06:30:22.002850Z","iopub.status.idle":"2023-08-09T06:30:35.416985Z","shell.execute_reply.started":"2023-08-09T06:30:22.002814Z","shell.execute_reply":"2023-08-09T06:30:35.415876Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"6756\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"     file_ids\n0  182_22.png\n1  167_27.png\n2   86_29.png\n3  154_16.png\n4   177_8.png","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file_ids</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>182_22.png</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>167_27.png</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>86_29.png</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>154_16.png</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>177_8.png</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport cv2\n\ndf = pd.read_csv('full_file_ids.csv')\nids = df['file_ids'].tolist()\nnon_empty_ids = []\n\nfor file_id in ids:\n    mask_path = os.path.join(label_path, 'L' + file_id)\n    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n    if cv2.countNonZero(mask) > 0:\n        non_empty_ids.append(file_id)\n\ndf_non_empty = pd.DataFrame(non_empty_ids, columns=[\"file_ids\"])\ndf_non_empty.sort_values(by='file_ids', inplace=True)  # Sort the DataFrame by 'file_ids'\ndf_non_empty.to_csv('file_ids.csv', index=False)\n\n\ndif = pd.read_csv('file_ids.csv')\ndif.head(15)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-09T06:30:35.421939Z","iopub.execute_input":"2023-08-09T06:30:35.422750Z","iopub.status.idle":"2023-08-09T06:30:58.067307Z","shell.execute_reply.started":"2023-08-09T06:30:35.422715Z","shell.execute_reply":"2023-08-09T06:30:58.066301Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"      file_ids\n0     0_10.png\n1     0_11.png\n2     0_12.png\n3     0_13.png\n4     0_14.png\n5     0_15.png\n6     0_16.png\n7     0_17.png\n8     0_18.png\n9     0_19.png\n10    0_20.png\n11    0_21.png\n12    0_22.png\n13  100_10.png\n14  100_11.png","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file_ids</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0_10.png</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0_11.png</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0_12.png</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0_13.png</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0_14.png</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0_15.png</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0_16.png</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0_17.png</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0_18.png</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0_19.png</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0_20.png</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0_21.png</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0_22.png</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>100_10.png</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>100_11.png</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"pip install 'transformers[torch]'","metadata":{"execution":{"iopub.status.busy":"2023-08-09T06:30:58.068756Z","iopub.execute_input":"2023-08-09T06:30:58.069774Z","iopub.status.idle":"2023-08-09T06:31:10.772743Z","shell.execute_reply.started":"2023-08-09T06:30:58.069740Z","shell.execute_reply":"2023-08-09T06:31:10.771432Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers[torch] in /opt/conda/lib/python3.10/site-packages (4.32.0.dev0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (3.12.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (5.4.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2023.5.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.28.2)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.3.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (4.64.1)\nRequirement already satisfied: torch!=1.12.0,>=1.9 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.0.0)\nCollecting accelerate>=0.20.3 (from transformers[torch])\n  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.3)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers[torch]) (2023.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers[torch]) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers[torch]) (3.0.9)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (2023.5.7)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch!=1.12.0,>=1.9->transformers[torch]) (2.1.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch!=1.12.0,>=1.9->transformers[torch]) (1.3.0)\nInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.12.0\n    Uninstalling accelerate-0.12.0:\n      Successfully uninstalled accelerate-0.12.0\nSuccessfully installed accelerate-0.21.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"class SegmentationDataset(Dataset):\n    def __init__(self, csv, processor):\n        self.df = pd.read_csv(csv)\n        self.ids = self.df[\"file_ids\"]\n        self.processor = processor\n        \n\n    def __getitem__(self, idx):\n        image = Image.open(\"/kaggle/input/nucleus-data-c-elegans/nucleus_data/features/F\" + self.ids[idx])\n        mask = np.array(Image.open(\"/kaggle/input/nucleus-data-c-elegans/nucleus_data/segmentation_maps/L\" + self.ids[idx]))\n           \n\n        \n\n        inputs = self.processor(image, return_tensors=\"pt\")\n        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n\n        inputs[\"ground_truth_mask\"] = mask\n\n        return inputs\n\n    def __len__(self):\n        return len(self.ids)\n               ","metadata":{"execution":{"iopub.status.busy":"2023-08-09T06:31:10.775105Z","iopub.execute_input":"2023-08-09T06:31:10.775602Z","iopub.status.idle":"2023-08-09T06:31:10.784964Z","shell.execute_reply.started":"2023-08-09T06:31:10.775555Z","shell.execute_reply":"2023-08-09T06:31:10.783707Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from transformers import SamProcessor\n\nprocessor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")","metadata":{"execution":{"iopub.status.busy":"2023-08-09T06:31:10.787002Z","iopub.execute_input":"2023-08-09T06:31:10.787274Z","iopub.status.idle":"2023-08-09T06:31:11.196120Z","shell.execute_reply.started":"2023-08-09T06:31:10.787250Z","shell.execute_reply":"2023-08-09T06:31:11.195182Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)rocessor_config.json:   0%|          | 0.00/466 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73eed20def7d41cb9c2fc00919810d39"}},"metadata":{}}]},{"cell_type":"code","source":"train_dataset = SegmentationDataset(csv = \"file_ids.csv\", processor=processor)","metadata":{"execution":{"iopub.status.busy":"2023-08-09T06:31:11.197583Z","iopub.execute_input":"2023-08-09T06:31:11.198613Z","iopub.status.idle":"2023-08-09T06:31:11.210659Z","shell.execute_reply.started":"2023-08-09T06:31:11.198578Z","shell.execute_reply":"2023-08-09T06:31:11.209683Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"example = train_dataset[0]\nfor k,v in example.items():\n  print(k,v.shape)","metadata":{"execution":{"iopub.status.busy":"2023-08-09T06:31:11.212143Z","iopub.execute_input":"2023-08-09T06:31:11.212917Z","iopub.status.idle":"2023-08-09T06:31:11.342549Z","shell.execute_reply.started":"2023-08-09T06:31:11.212886Z","shell.execute_reply":"2023-08-09T06:31:11.341433Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"pixel_values torch.Size([3, 1024, 1024])\noriginal_sizes torch.Size([2])\nreshaped_input_sizes torch.Size([2])\nground_truth_mask (256, 256)\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n    ","metadata":{"execution":{"iopub.status.busy":"2023-08-09T07:14:38.451471Z","iopub.execute_input":"2023-08-09T07:14:38.451825Z","iopub.status.idle":"2023-08-09T07:14:38.457001Z","shell.execute_reply.started":"2023-08-09T07:14:38.451796Z","shell.execute_reply":"2023-08-09T07:14:38.455908Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"batch = next(iter(train_dataloader))\nfor k,v in batch.items():\n  print(k,v.shape)","metadata":{"execution":{"iopub.status.busy":"2023-08-09T07:14:38.459040Z","iopub.execute_input":"2023-08-09T07:14:38.459736Z","iopub.status.idle":"2023-08-09T07:14:38.558501Z","shell.execute_reply.started":"2023-08-09T07:14:38.459699Z","shell.execute_reply":"2023-08-09T07:14:38.557416Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"pixel_values torch.Size([1, 3, 1024, 1024])\noriginal_sizes torch.Size([1, 2])\nreshaped_input_sizes torch.Size([1, 2])\nground_truth_mask torch.Size([1, 256, 256])\n","output_type":"stream"}]},{"cell_type":"code","source":"batch[\"ground_truth_mask\"].shape\n","metadata":{"execution":{"iopub.status.busy":"2023-08-09T07:14:38.561490Z","iopub.execute_input":"2023-08-09T07:14:38.561903Z","iopub.status.idle":"2023-08-09T07:14:38.568483Z","shell.execute_reply.started":"2023-08-09T07:14:38.561866Z","shell.execute_reply":"2023-08-09T07:14:38.567550Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 256, 256])"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import SamModel \n\nmodel = SamModel.from_pretrained(\"facebook/sam-vit-base\")\n","metadata":{"execution":{"iopub.status.busy":"2023-08-09T07:14:38.570214Z","iopub.execute_input":"2023-08-09T07:14:38.570990Z","iopub.status.idle":"2023-08-09T07:14:39.931882Z","shell.execute_reply.started":"2023-08-09T07:14:38.570958Z","shell.execute_reply":"2023-08-09T07:14:39.930836Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"for name, param in model.named_parameters():\n  if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n    param.requires_grad_(False)","metadata":{"execution":{"iopub.status.busy":"2023-08-09T07:14:39.934835Z","iopub.execute_input":"2023-08-09T07:14:39.935228Z","iopub.status.idle":"2023-08-09T07:14:39.942550Z","shell.execute_reply.started":"2023-08-09T07:14:39.935192Z","shell.execute_reply":"2023-08-09T07:14:39.941673Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"from torch.optim import Adam\nimport monai\n\n# Note: Hyperparameter tuning could improve performance here\noptimizer = Adam(model.mask_decoder.parameters(), lr=1e-5, weight_decay=0)\n\nseg_loss = monai.losses.DiceCELoss(sigmoid=True, squared_pred=True, reduction='mean')\n","metadata":{"execution":{"iopub.status.busy":"2023-08-09T07:14:39.944161Z","iopub.execute_input":"2023-08-09T07:14:39.944850Z","iopub.status.idle":"2023-08-09T07:14:39.954010Z","shell.execute_reply.started":"2023-08-09T07:14:39.944817Z","shell.execute_reply":"2023-08-09T07:14:39.952937Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# !pip install torch-summary","metadata":{"execution":{"iopub.status.busy":"2023-08-09T07:14:39.955292Z","iopub.execute_input":"2023-08-09T07:14:39.956346Z","iopub.status.idle":"2023-08-09T07:14:39.964339Z","shell.execute_reply.started":"2023-08-09T07:14:39.956309Z","shell.execute_reply":"2023-08-09T07:14:39.963304Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# from torchsummary import summary\n# summary(model,(3, 1024, 1024))","metadata":{"execution":{"iopub.status.busy":"2023-08-09T07:14:39.967476Z","iopub.execute_input":"2023-08-09T07:14:39.967756Z","iopub.status.idle":"2023-08-09T07:14:39.975534Z","shell.execute_reply.started":"2023-08-09T07:14:39.967732Z","shell.execute_reply":"2023-08-09T07:14:39.974448Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# !nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2023-08-09T07:14:39.976850Z","iopub.execute_input":"2023-08-09T07:14:39.977274Z","iopub.status.idle":"2023-08-09T07:14:39.989052Z","shell.execute_reply.started":"2023-08-09T07:14:39.977242Z","shell.execute_reply":"2023-08-09T07:14:39.988104Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nfrom statistics import mean\nimport torch\nfrom torch.nn.functional import threshold, normalize\n\nnum_epochs = 100\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\n\nmodel.train()\nfor epoch in range(num_epochs):\n    epoch_losses = []\n    for batch in tqdm(train_dataloader):\n        \n        outputs = model(\n            pixel_values=batch[\"pixel_values\"].to(device),\n            input_masks=batch[\"ground_truth_mask\"].float().to(device),\n            multimask_output=False,\n        )\n\n        # compute loss\n        predicted_masks = outputs.pred_masks\n        predicted_mask = torch.squeeze(predicted_masks, dim=1)\n        ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n        loss = seg_loss(predicted_mask, ground_truth_masks.unsqueeze(dim=1))\n\n        # backward pass (compute gradients of parameters w.r.t. loss)\n        optimizer.zero_grad()\n        loss.backward()\n\n        # optimize\n        optimizer.step()\n        epoch_losses.append(loss.item())\n\n    print(f\"EPOCH: {epoch}\")\n    print(f\"Mean loss: {mean(epoch_losses)}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-08-09T07:15:34.476595Z","iopub.execute_input":"2023-08-09T07:15:34.476967Z","iopub.status.idle":"2023-08-09T08:11:30.686933Z","shell.execute_reply.started":"2023-08-09T07:15:34.476934Z","shell.execute_reply":"2023-08-09T08:11:30.685335Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stderr","text":"100%|██████████| 4978/4978 [27:49<00:00,  2.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 0\nMean loss: 0.9956125541473786\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4978/4978 [27:27<00:00,  3.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 1\nMean loss: 0.9956059450722928\n","output_type":"stream"},{"name":"stderr","text":"  2%|▏         | 118/4978 [00:39<26:47,  3.02it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[41], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m epoch_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_dataloader):\n\u001b[0;32m---> 16\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpixel_values\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_masks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mground_truth_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmultimask_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# compute loss\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     predicted_masks \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mpred_masks\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/sam/modeling_sam.py:1368\u001b[0m, in \u001b[0;36mSamModel.forward\u001b[0;34m(self, pixel_values, input_points, input_labels, input_boxes, input_masks, image_embeddings, multimask_output, attention_similarity, target_embedding, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1365\u001b[0m vision_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pixel_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1368\u001b[0m     vision_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_encoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1370\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1374\u001b[0m     image_embeddings \u001b[38;5;241m=\u001b[39m vision_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/sam/modeling_sam.py:1057\u001b[0m, in \u001b[0;36mSamVisionEncoder.forward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1052\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m   1053\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m   1054\u001b[0m         hidden_states,\n\u001b[1;32m   1055\u001b[0m     )\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1057\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1059\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/sam/modeling_sam.py:943\u001b[0m, in \u001b[0;36mSamVisionLayer.forward\u001b[0;34m(self, hidden_states, output_attentions)\u001b[0m\n\u001b[1;32m    940\u001b[0m     height, width \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], hidden_states\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    941\u001b[0m     hidden_states, padding_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_partition(hidden_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size)\n\u001b[0;32m--> 943\u001b[0m hidden_states, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;66;03m# Reverse window partition\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/sam/modeling_sam.py:846\u001b[0m, in \u001b[0;36mSamVisionAttention.forward\u001b[0;34m(self, hidden_states, output_attentions)\u001b[0m\n\u001b[1;32m    843\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m (query \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale) \u001b[38;5;241m@\u001b[39m key\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    845\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_rel_pos:\n\u001b[0;32m--> 846\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_decomposed_rel_pos\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel_pos_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel_pos_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    850\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(attn_weights, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(query\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    852\u001b[0m attn_probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(attn_weights, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/sam/modeling_sam.py:820\u001b[0m, in \u001b[0;36mSamVisionAttention.add_decomposed_rel_pos\u001b[0;34m(self, attn, query, rel_pos_h, rel_pos_w, q_size, k_size)\u001b[0m\n\u001b[1;32m    818\u001b[0m query_height, query_width \u001b[38;5;241m=\u001b[39m q_size\n\u001b[1;32m    819\u001b[0m key_height, key_width \u001b[38;5;241m=\u001b[39m k_size\n\u001b[0;32m--> 820\u001b[0m relative_position_height \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_rel_pos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrel_pos_h\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m relative_position_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_rel_pos(query_width, key_width, rel_pos_w)\n\u001b[1;32m    823\u001b[0m batch_size, _, dim \u001b[38;5;241m=\u001b[39m query\u001b[38;5;241m.\u001b[39mshape\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"print(torch.cuda.memory_summary(device=None, abbreviated=False))","metadata":{"execution":{"iopub.status.busy":"2023-08-09T07:14:40.616763Z","iopub.status.idle":"2023-08-09T07:14:40.617890Z","shell.execute_reply.started":"2023-08-09T07:14:40.617624Z","shell.execute_reply":"2023-08-09T07:14:40.617647Z"},"trusted":true},"execution_count":null,"outputs":[]}]}